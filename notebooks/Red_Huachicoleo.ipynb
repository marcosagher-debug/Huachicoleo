{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección robusta de robo de hidrocarburos con Autoencoder LSTM\n",
    "\n",
    "Este cuaderno implementa un pipeline reproducible para entrenar, calibrar y evaluar un autoencoder LSTM sobre datos de operación de bombas de rebombeo. El enfoque está optimizado para detectar comportamientos anómalos asociados al robo de hidrocarburos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
    "                             precision_score, recall_score,\n",
    "                             roc_auc_score, average_precision_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración global reproducible\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar y ordenar los datos\n",
    "DATA_PATH = Path('../data/rebombeo_huachicoleo.csv')\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['timestamp']).sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"Total de registros: {len(df):,}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir hiperparámetros y utilidades de generación de ventanas\n",
    "WINDOW = 30  # minutos por ventana\n",
    "STEP = 5     # salto entre ventanas\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "FEATURES = [\"flow\", \"pressure\", \"pump_rpm\", \"tank_level\", \"power\"]\n",
    "\n",
    "\n",
    "def generate_windows(values: np.ndarray, labels: np.ndarray, timestamps: pd.Series,\n",
    "                     window: int, step: int):\n",
    "    \"\"\"Genera ventanas deslizantes, etiquetas agregadas y el timestamp final de cada ventana.\"\"\"\n",
    "    X, y, ts = [], [], []\n",
    "    for start in range(0, len(values) - window + 1, step):\n",
    "        end = start + window\n",
    "        X.append(values[start:end])\n",
    "        y.append(int(labels[start:end].max()))\n",
    "        ts.append(timestamps.iloc[end - 1])\n",
    "    return np.array(X), np.array(y), pd.Series(ts, name='window_end')\n",
    "\n",
    "\n",
    "def build_split(start_idx: int, end_idx: int):\n",
    "    \"\"\"Construye ventanas dentro del intervalo [start_idx, end_idx).\"\"\"\n",
    "    values = scaled_values[start_idx:end_idx]\n",
    "    labels = label_array[start_idx:end_idx]\n",
    "    timestamps = df.loc[start_idx:end_idx - 1, 'timestamp']\n",
    "    return generate_windows(values, labels, timestamps, WINDOW, STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir los datos por tiempo y escalar usando únicamente registros normales de entrenamiento\n",
    "n_samples = len(df)\n",
    "train_end = int(n_samples * TRAIN_RATIO)\n",
    "val_end = int(n_samples * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "df_train = df.iloc[:train_end]\n",
    "df_val = df.iloc[train_end:val_end]\n",
    "df_test = df.iloc[val_end:]\n",
    "\n",
    "label_array = df['label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_normal_mask = (df.index < train_end) & (df['label'] == 0)\n",
    "scaler.fit(df.loc[train_normal_mask, FEATURES])\n",
    "scaled_values = scaler.transform(df[FEATURES])\n",
    "\n",
    "print(f\"Registros entrenamiento: {len(df_train):,} | validación: {len(df_val):,} | prueba: {len(df_test):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear ventanas para cada partición temporal\n",
    "X_train_all, y_train_all, ts_train = build_split(0, train_end)\n",
    "X_val, y_val, ts_val = build_split(train_end, val_end)\n",
    "X_test, y_test, ts_test = build_split(val_end, n_samples)\n",
    "\n",
    "# Solo ventanas normales para entrenamiento del autoencoder\n",
    "X_train = X_train_all[y_train_all == 0]\n",
    "\n",
    "print(f\"Ventanas de entrenamiento: {len(X_train):,} (solo normales)\")\n",
    "print(f\"Ventanas de validación: {len(X_val):,} | Ventanas de prueba: {len(X_test):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir arquitectura del Autoencoder LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "TIMESTEPS = X_train.shape[1]\n",
    "N_FEATURES = X_train.shape[2]\n",
    "\n",
    "inputs = Input(shape=(TIMESTEPS, N_FEATURES))\n",
    "encoded = LSTM(64, activation='tanh', return_sequences=True, dropout=0.1)(inputs)\n",
    "encoded = LSTM(32, activation='tanh', dropout=0.1)(encoded)\n",
    "\n",
    "latent = RepeatVector(TIMESTEPS)(encoded)\n",
    "\n",
    "decoded = LSTM(32, activation='tanh', return_sequences=True, dropout=0.1)(latent)\n",
    "decoded = LSTM(64, activation='tanh', return_sequences=True, dropout=0.1)(decoded)\n",
    "outputs = TimeDistributed(Dense(N_FEATURES))(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, outputs, name='lstm_autoencoder')\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenar el modelo con parada temprana basada en validación reconstructiva\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Pérdida entrenamiento')\n",
    "plt.plot(history.history['val_loss'], label='Pérdida validación')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Curvas de entrenamiento del autoencoder')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funciones auxiliares para evaluación\n",
    "\n",
    "def reconstruction_errors(model, X):\n",
    "    reconstructions = model.predict(X, verbose=0)\n",
    "    return np.mean(np.square(X - reconstructions), axis=(1, 2))\n",
    "\n",
    "\n",
    "def calibrate_threshold(errors_normals: np.ndarray, errors_val: np.ndarray, labels_val: np.ndarray):\n",
    "    candidate_percentiles = np.linspace(80, 99, 40)\n",
    "    best_threshold, best_f1 = None, -np.inf\n",
    "    metrics = []\n",
    "    for p in candidate_percentiles:\n",
    "        threshold = np.percentile(errors_normals, p)\n",
    "        preds = (errors_val > threshold).astype(int)\n",
    "        f1 = f1_score(labels_val, preds, zero_division=0)\n",
    "        precision = precision_score(labels_val, preds, zero_division=0)\n",
    "        recall = recall_score(labels_val, preds, zero_division=0)\n",
    "        metrics.append({'percentil': p, 'threshold': threshold, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "        if f1 > best_f1:\n",
    "            best_threshold, best_f1 = threshold, f1\n",
    "    return best_threshold, pd.DataFrame(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calibrar umbral con el conjunto de validación\n",
    "val_errors = reconstruction_errors(autoencoder, X_val)\n",
    "val_normal_errors = val_errors[y_val == 0]\n",
    "\n",
    "threshold, calibration_df = calibrate_threshold(val_normal_errors, val_errors, y_val)\n",
    "\n",
    "print(f\"Umbral óptimo (max F1 en validación): {threshold:.6f}\")\n",
    "calibration_df.sort_values('f1', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluar desempeño en el conjunto de prueba\n",
    "train_errors = reconstruction_errors(autoencoder, X_train)\n",
    "test_errors = reconstruction_errors(autoencoder, X_test)\n",
    "\n",
    "y_pred_test = (test_errors > threshold).astype(int)\n",
    "\n",
    "report = classification_report(y_test, y_pred_test, target_names=['Normal', 'Anomalía'], zero_division=0, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "roc_auc = roc_auc_score(y_test, test_errors)\n",
    "avg_precision = average_precision_score(y_test, test_errors)\n",
    "\n",
    "print(\"Métricas de clasificación (prueba):\")\n",
    "display(report_df)\n",
    "print(f\"ROC AUC (errores): {roc_auc:.3f}\")\n",
    "print(f\"Average Precision: {avg_precision:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicción Normal', 'Predicción Anomalía'],\n",
    "            yticklabels=['Real Normal', 'Real Anomalía'])\n",
    "plt.title('Matriz de confusión - prueba')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicción')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizaciones complementarias\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(ts_test, test_errors, label='Error reconstrucción')\n",
    "ax.axhline(threshold, color='red', linestyle='--', label='Umbral calibrado')\n",
    "ax.set_xlabel('Timestamp fin de ventana')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Errores de reconstrucción en prueba')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.kdeplot(train_errors, label='Entrenamiento (normal)')\n",
    "sns.kdeplot(test_errors[y_test == 0], label='Prueba - normales')\n",
    "sns.kdeplot(test_errors[y_test == 1], label='Prueba - anomalías')\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Umbral')\n",
    "plt.xlabel('Error de reconstrucción')\n",
    "plt.ylabel('Densidad')\n",
    "plt.title('Distribución de errores por conjunto')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de acciones recomendadas\n",
    "\n",
    "- **Pipeline reproducible:** separación temporal y escalado basados únicamente en datos normales de entrenamiento.\n",
    "- **Calibración basada en validación:** el umbral se selecciona maximizando F1 y equilibrando precisión/recall.\n",
    "- **Métricas auditables:** se reportan F1, precisión, recall, ROC-AUC y Average Precision en el conjunto de prueba, junto con matrices de confusión y distribuciones de error.\n",
    "\n",
    "Este flujo deja listos los componentes para empaquetar el modelo, escalarlo a producción y generar reportes periódicos de desempeño."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}